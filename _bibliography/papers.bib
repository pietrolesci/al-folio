@inproceedings{lesci-etal-2023-diable,
  abstract    = {Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to noisy data annotations due to the table operations approach.},
  address     = {Toronto, Canada},
  arxiv       = {https://arxiv.org/abs/2305.17020},
  author      = {Lesci, Pietro  and
                 Fujinuma, Yoshinari  and
                 Hardalov, Momchil  and
                 Shang, Chao  and
                 Benajiba, Yassine  and
                 Marquez, Lluis},
  bibtex_show = {},
  booktitle   = {Findings of the Association for Computational Linguistics: ACL 2023},
  code        = {https://github.com/amazon-science/efficient-dialogue-state-tracking-by-sequential-information-processing},
  doi         = {10.18653/v1/2023.findings-acl.615},
  editor      = {Rogers, Anna  and
                 Boyd-Graber, Jordan  and
                 Okazaki, Naoaki},
  month       = {jul},
  pages       = {9697--9719},
  publisher   = {Association for Computational Linguistics},
  title       = {Diable: Efficient Dialogue State Tracking as Operations on Tables},
  url         = {https://aclanthology.org/2023.findings-acl.615},
  year        = {2023},
}

@inproceedings{lesci-etal-2024-causal,
  abstract    = {Understanding memorisation in language models has practical and societal implications, e.g., studying models' training dynamics or preventing copyright infringements.
                 Previous work defines memorisation as the causal effect of training with an instance on the model's ability to predict it. 
                 This definition relies on a counterfactual: the ability to observe what would have happened had the model not seen that instance.
                 Existing methods struggle to provide computationally efficient and accurate estimates of this counterfactual. 
                 Further, they often resort to estimating memorisation for a model architecture rather than for a specific model instance. 
                 This paper fills an important gap in the literature, proposing a new, theoretically principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics. 
                 Using this method, we can characterise a model's memorisation profile--its memorisation trends throughout training--by only observing its behaviour on a small set of instances throughout training.
                 In experiments with the Pythia model suite, we find that memorisation is higher and more persistent in larger models, and that it is strongly determined by data order and learning rate.},
  arxiv       = {https://arxiv.org/abs/2406.04327},
  author      = {Pietro Lesci and Clara Meister and Thomas Hofmann and Andreas Vlachos and Tiago Pimentel},
  bibtex_show = {},
  booktitle   = {Proceedings of the The 62nd Annual Meeting of the Association for Computational Linguistics 2024, Bangkok, Thailand August 11-16, 2024},
  code        = {https://github.com/pietrolesci/memorisation-profiles},
  pages       = {to appear},
  publisher   = {Association for Computational Linguistics},
  title       = {Causal Estimation of Memorisation Profiles},
  year        = {2024},
}


@inproceedings{lesci-vlachos-2024-anchoral,
  abstract    = {Active learning for imbalanced classification tasks is challenging as the minority classes naturally occur rarely. Gathering a large pool of unlabelled data is thus essential to capture minority instances. Standard pool-based active learning is computationally expensive on large pools and often reaches low accuracy by overfitting the initial decision boundary, thus failing to explore the input space and find minority instances. To address these issues we propose AnchorAL. At each iteration, AnchorAL chooses class-specific instances from the labelled set, or anchors, and retrieves the most similar unlabelled instances from the pool. This resulting subpool is then used for active learning. Using a small, fixed-sized subpool AnchorAL allows scaling any active learning strategy to large pools. By dynamically selecting different anchors at each iteration it promotes class balance and prevents overfitting the initial decision boundary, thus promoting the discovery of new clusters of minority instances. Experiments across different classification tasks, active learning strategies, and model architectures AnchorAL is (i) faster, often reducing runtime from hours to minutes, (ii) trains more performant models, (iii) and returns more balanced datasets than competing methods.},
  address     = {Mexico City, Mexico},
  arxiv       = {https://arxiv.org/abs/2404.05623},
  author      = {Lesci, Pietro and Vlachos, Andreas},
  bibtex_show = {},
  booktitle   = {Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  code        = {https://github.com/pietrolesci/anchoral},
  doi         = {https://doi.org/10.48550/arXiv.2404.05623},
  editor      = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
  month       = jun,
  pages       = {8445--8464},
  publisher   = {Association for Computational Linguistics},
  title       = {{AnchorAL}: {C}omputationally Efficient Active Learning for Large and Imbalanced Datasets},
  url         = {https://aclanthology.org/2024.naacl-long.467},
  year        = {2024},
}


@inproceedings{diehl-martinez-etal-2024-tending,
  title    = {Tending Towards Stability: {C}onvergence Challenges in Small Language Models},
  address     = {Miami, Florida, USA},
  author      = {Diehl Martinez, Richard and Lesci, Pietro and Buttery, Paula},
  booktitle   = {To appear in Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  abstract       = {Increasing the number of parameters in language models is a common strategy to enhance their performance. However, smaller language models remain valuable due to their lower operational costs. Despite their advantages, smaller models frequently underperform compared to their larger counterparts, even when provided with equivalent data and computational resources. Specifically, their performance tends to degrade in the late pretraining phase. This is anecdotally attributed to their reduced representational capacity. Yet, the exact causes of this performance degradation remain unclear. We use the Pythia model suite to analyse the training dynamics that underlie this phenomenon. Across different model sizes, we investigate the convergence of the Attention and MLP activations to their final state and examine how the effective rank of their parameters influences this process. We find that nearly all layers in larger models stabilise early in training—within the first 20%—whereas layers in smaller models exhibit slower and less stable convergence, especially when their parameters have lower effective rank. By linking the convergence of layers' activations to their parameters' effective rank, our analyses can guide future work to address inefficiencies in the learning dynamics of small models.},
  year        = {2024},
}